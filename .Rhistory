x <- torch_rand(c(2, 4, 3, 16, 16)) # batch_size, seq_len, channels, height, width
## Sanity Check
dl <- create_dummy_data()
preds <- train_convlstm(dl = dl, 10, plot_path = "vignettes/learning_curve-sanity.png")
dl
dl %>% size()
dl %>% dim()
preds
preds %>% dim()
dl
dl$sampler
dl %>% length()
cube <- readRDS("~/msc-thesis/cube_ch.rds")
train_dl <- cube[1]
test_dl <- cube[2]
preds <- train_convlstm(train_dl, 3)
train_dl
cube
## Wrangle Cube to tensor
cube = create_dl_from_cube()
cube
train_dl <- cube[1]
test_dl <- cube[2]
preds <- train_convlstm(train_dl, 3)
train_dl
test_dl
cube
train_dl <- cube[[1]]
test_dl <- cube[[2]]
train_dl
preds <- train_convlstm(train_dl, 3)
library(eoforecast)
## convLSTM
library(eoforecast)
# Quick Training for sanity check
preds <- train_convlstm(train_dl, 3)
# dummy input tensor
x <- torch_rand(c(2, 4, 3, 16, 16)) # batch_size, seq_len, channels, height, width
## Sanity Check
dl <- create_dummy_data()
preds <- train_convlstm(dl = dl, 10, plot_path = "vignettes/learning_curve-sanity.png")
## Tiny convlstm
train_convlstm <- function(dl,
num_epochs = 100,
plot_path = "vignettes/learning_curve.png",
input_dim = 1,
hidden_dims = c(64, 1),
kernel_sizes = c(3, 3),
n_layers = 2){
model <- convlstm(input_dim = input_dim, hidden_dims = hidden_dims, kernel_sizes = kernel_sizes, n_layers = n_layers)
### Adam optimizer
optimizer <- optim_adam(model$parameters)
losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
model$train()
batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
batch_losses <- c(batch_losses, loss$item())
loss$backward()
optimizer$step()
})
if (epoch %% 10 == 0)
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses)))
cli::cli_progress_update()
}
cli::cli_progress_done()
# create learning rate plot
learning_curve <- tibble(epoch = 1:num_epochs, loss = losses) %>%
ggplot(aes(x = epoch, y = loss)) +
geom_line() +
xlab("Epochs") +
ylab("Loss") +
ggtitle("Learning Rate Curve")
ggsave(plot_path, learning_curve)
return(preds)
}
train_dl
train_dl <- cube[[1]]
train_dl
cube
## Wrangle Cube to tensor
cube = create_dl_from_cube()
train_dl <- cube[[1]]
test_dl <- cube[[2]]
train_dl
# Quick Training for sanity check
preds <- train_convlstm(train_dl, 3)
rlang::last_trace()
library(eoforecast)
## convLSTM
library(eoforecast)
# dummy input tensor
x <- torch_rand(c(2, 4, 3, 16, 16)) # batch_size, seq_len, channels, height, width
## Sanity Check
dl <- create_dummy_data()
## Wrangle Cube to tensor
cube = create_dl_from_cube()
library(eoforecast)
install.packages("openeo")
library(eoforecast)
library(openeo)
install.packages("sf")
library(eoforecast)
install.packages("openeo")
library(eoforecast)
library(openeo)
remove.packages("openeo")
remove.packages("sf")
.libPaths()
install.packages("sf")
install.packages("openeo")
library(sfheaders)
library(sf)
library(sf)
install.packages("sf", dep = T)
library(sf)
install.packages("sf", dep = T)
install.packages("satellite", dep = T)
library(eoforecast)
## convLSTM
library(eoforecast)
device <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
## Wrangle Cube to tensor
cube <- create_dl_from_cube()
# Quick Training for sanity check
preds <- train_convlstm(cube, num_epochs = 6)
library(eoforecast)
# Quick Training for sanity check
preds <- train_convlstm(cube, num_epochs = 6)
## convLSTM
library(eoforecast)
## Wrangle Cube to tensor
cube <- create_dl_from_cube()
# Quick Training for sanity check
preds <- train_convlstm(cube, num_epochs = 6)
num_epochs = 10
plot_path = "vignettes/learning_curve.png"
input_dim = 1
hidden_dims = c(64, 1)
kernel_sizes = c(3, 3)
n_layers = 2
.device = device
device
device <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
device
library(eoforecast)
# Quick Training for sanity check
preds <- train_convlstm(cube, num_epochs = 6)
# Quick Training for sanity check
preds <- train_convlstm(cube, num_epochs = 6, .device = device)
device
device
device <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
device
dev <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
# Quick Training for sanity check
preds <- train_convlstm(cube, num_epochs = 6, .device = dev)
num_epochs
dl
dl = cube
model <- convlstm(input_dim = input_dim, hidden_dims = hidden_dims, kernel_sizes = kernel_sizes, n_layers = n_layers)
model <- model$to(device = .device)
.device
.device = dev
.device
model <- model$to(device = .device)
### Adam optimizer
optimizer <- optim_adam(model$parameters)
losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
# train_losses <- c(train_losses, loss$item)
})
# Early stopping
if (epoch > 10){
if (losses[epoch] >= losses[1]){
stop(cli::cli_abort("Early Stopping triggered"))
}
}
if (epoch %% 10 == 0)
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
model$eval()
}
dl
cube
## Wrangle Cube to tensor
cube <- create_dl_from_cube()
cube
# Quick Training for sanity check
preds <- train_convlstm(cube, num_epochs = 6, .device = dev)
losses
num_epochs
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
# train_losses <- c(train_losses, loss$item)
})
# Early stopping
if (epoch > 10){
if (losses[epoch] >= losses[1]){
stop(cli::cli_abort("Early Stopping triggered"))
}
}
if (epoch %% 10 == 0)
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
model$eval()
}
cube
dl = cube
losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
# train_losses <- c(train_losses, loss$item)
})
# Early stopping
if (epoch > 10){
if (losses[epoch] >= losses[1]){
stop(cli::cli_abort("Early Stopping triggered"))
}
}
if (epoch %% 10 == 0)
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
model$eval()
}
cli::cli_progress_done()
losses <- zoo::rollapply(losses, width = 3, FUN = mean, by = 3, align = "left", fill = NA) %>% na.omit()
losses
tibble(epoch = 1:num_epochs, loss = losses)
losses
losses
batch_losses
losses
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
batch_losses <- c(batch_losses, loss$item)
})
# Early stopping
if (epoch > 10){
if (losses[epoch] >= losses[1]){
stop(cli::cli_abort("Early Stopping triggered"))
}
}
if (epoch %% 10 == 0)
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
model$eval()
}
### Adam optimizer
optimizer <- optim_adam(model$parameters)
losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
batch_losses <- c(batch_losses, loss$item)
})
# Early stopping
if (epoch > 10){
if (losses[epoch] >= losses[1]){
stop(cli::cli_abort("Early Stopping triggered"))
}
}
if (epoch %% 10 == 0)
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
model$eval()
}
cli::cli_progress_done()
losses
zoo::rollapply(losses, width = 3, FUN = mean, by = 3, align = "left", fill = NA) %>% na.omit()
batch_losses
?rollapply
train_losses <- c()
val_losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
train_batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
train_batch_losses <- c(train_batch_losses, loss$item)
})
mean_train_loss <- mean(train_batch_losses, na.rm = TRUE)
train_losses <- c(train_losses, mean_train_loss)
cli::cli_alert(paste("Epoch", epoch, "Training Loss:", round(mean_train_loss, 3)))
# Validation loss
model$eval()
val_batch_losses <- c()
coro::loop(for (b in val_dl) {
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
val_batch_losses <- c(val_batch_losses, loss$item)
})
mean_val_loss <- mean(val_batch_losses, na.rm = TRUE)
val_losses <- c(val_losses, mean_val_loss)
cli::cli_alert(paste("Epoch", epoch, "Validation Loss:", round(mean_val_loss, 3)))
# Early stopping
if (epoch > 10 && mean_val_loss >= val_losses[1]) {
stop(cli::cli_abort("Early Stopping triggered"))
}
if (epoch %% 10 == 0) {
cat(sprintf("\nEpoch %d, training loss:%3f, validation loss:%3f\n", epoch, mean_train_loss, mean_val_loss))
}
}
base::mean
train_losses <- c()
val_losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
train_batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
train_batch_losses <- c(train_batch_losses, loss$item)
})
mean_train_loss <- base::mean(train_batch_losses, na.rm = TRUE)
train_losses <- c(train_losses, mean_train_loss)
cli::cli_alert(paste("Epoch", epoch, "Training Loss:", round(mean_train_loss, 3)))
# Validation loss
model$eval()
val_batch_losses <- c()
coro::loop(for (b in val_dl) {
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
val_batch_losses <- c(val_batch_losses, loss$item)
})
mean_val_loss <- base::mean(val_batch_losses, na.rm = TRUE)
val_losses <- c(val_losses, mean_val_loss)
cli::cli_alert(paste("Epoch", epoch, "Validation Loss:", round(mean_val_loss, 3)))
# Early stopping
if (epoch > 10 && mean_val_loss >= val_losses[1]) {
stop(cli::cli_abort("Early Stopping triggered"))
}
if (epoch %% 10 == 0) {
cat(sprintf("\nEpoch %d, training loss:%3f, validation loss:%3f\n", epoch, mean_train_loss, mean_val_loss))
}
}
## convLSTM
library(eoforecast)
dev <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
## Wrangle Cube to tensor
cube <- create_dl_from_cube()
model <- convlstm(input_dim = input_dim, hidden_dims = hidden_dims, kernel_sizes = kernel_sizes, n_layers = n_layers)
model <- model$to(device = .device)
### Adam optimizer
optimizer <- optim_adam(model$parameters)
train_losses <- c()
val_losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
train_batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
train_batch_losses <- c(train_batch_losses, loss$item)
})
mean_train_loss <- base::mean(train_batch_losses, na.rm = TRUE)
train_losses <- c(train_losses, mean_train_loss)
cli::cli_alert(paste("Epoch", epoch, "Training Loss:", round(mean_train_loss, 3)))
# Validation loss
model$eval()
val_batch_losses <- c()
coro::loop(for (b in val_dl) {
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
val_batch_losses <- c(val_batch_losses, loss$item)
})
mean_val_loss <- base::mean(val_batch_losses, na.rm = TRUE)
val_losses <- c(val_losses, mean_val_loss)
cli::cli_alert(paste("Epoch", epoch, "Validation Loss:", round(mean_val_loss, 3)))
# Early stopping
if (epoch > 10 && mean_val_loss >= val_losses[1]) {
stop(cli::cli_abort("Early Stopping triggered"))
}
if (epoch %% 10 == 0) {
cat(sprintf("\nEpoch %d, training loss:%3f, validation loss:%3f\n", epoch, mean_train_loss, mean_val_loss))
}
}
openeo::connect()
openeo::connect("openeo.cloud")
for (epoch in 1:num_epochs) {
cli::cli_progress_update()
model$train()
batch_losses <- c()
train_batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
loss$backward()
optimizer$step()
train_batch_losses <- c(train_batch_losses, loss$item)
})
mean_train_loss <- base::mean(train_batch_losses, na.rm = TRUE)
train_losses <- c(train_losses, mean_train_loss)
cli::cli_alert(paste("Epoch", epoch, "Training Loss:", round(mean_train_loss, 3)))
# Validation loss
model$eval()
val_batch_losses <- c()
coro::loop(for (b in val_dl) {
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
val_batch_losses <- c(val_batch_losses, loss$item)
})
mean_val_loss <- base::mean(val_batch_losses, na.rm = TRUE)
val_losses <- c(val_losses, mean_val_loss)
cli::cli_alert(paste("Epoch", epoch, "Validation Loss:", round(mean_val_loss, 3)))
# Early stopping
if (epoch > 10 && mean_val_loss >= val_losses[1]) {
stop(cli::cli_abort("Early Stopping triggered"))
}
if (epoch %% 10 == 0) {
cat(sprintf("\nEpoch %d, training loss:%3f, validation loss:%3f\n", epoch, mean_train_loss, mean_val_loss))
}
}
