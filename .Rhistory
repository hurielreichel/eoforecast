labs(x = "X", y = "Y", title = "NO2 0-1") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = guide_colorbar(title = "NO2 Prediction"))
ggplot(prd_df, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_viridis_c() +
labs(x = "X", y = "Y", title = "NO2 0-1") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = guide_colorbar(title = "NO2 Prediction"))
ggplot(prd_df, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_viridis_c(limits = c(0,.4)) +
labs(x = "X", y = "Y", title = "NO2 0-1") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = guide_colorbar(title = "NO2 Prediction"))
# plot the real value
# Reshape the matrix for ggplot
prd_df <- melt(val)
ggplot(prd_df, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_viridis_c(limits = c(0,.4)) +
labs(x = "X", y = "Y", title = "NO2 0-1") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = guide_colorbar(title = "NO2 Real Value"))
# plot the error
# Reshape the matrix for ggplot
mat_df <- melt(prd-val)
rmse = (mat_df$value ** 2) %>% sqrt() %>% mean() %>% round(2)
# Create a ggplot object
ggplot(mat_df, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", mid = "white", midpoint = 0, high = "red", limits = c(-.5, .5)) +
labs(x = "X", y = "Y", title = paste("Error Map - RMSE=", as.character(rmse))) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = guide_colorbar(title = "Error"))
prd
cube_window
cube_window[[1]][1,,]
cube_window[[1]][1,,] %>% image()
cube_window[[1]][1,,] %>% as.matrix() %>% image()
cube_window[[30]][1,,] %>% as.matrix() %>% image()
# Create an initial sequence the size of the window
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
cube_window[[i]] <- cube_window[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
cube_window[[i]][is.na(cube_window[[i]])] <- 0
cube_window[[i]][is.nan(cube_window[[i]])] <- 0
cube_window[[i]][cube_window[[i]]<0] <- 0
cube_window[[i]][cube_window[[i]]>100] <- 0 # rough outlier removal
cube_window[[i]] <- cube_window[[i]] / max_v # scaling from 0 to 1 (original_value - min_value) / (max_value - min_value)
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
cube_window[[30]][1,,] %>% as.matrix() %>% image()
cube_window[[30]] %>% as.matrix() %>% image()
cube_window[[1]] %>% as.matrix() %>% image()
cube_window[[30]] %>% as.matrix() %>% image()
max_weather
cube_array <- array(0, dim = c(window_size, 2, ncol(raster::raster(file_paths[[1]])), nrow(raster::raster(file_paths[[1]]))))
cube_window <- vector("list", window_size)
for (i in 1:window_size){
no2_data <- read_stars(file_paths[[length(file_paths)-window_size+i]]) %>% pull()
no2_data <- no2_data[,40:1]
no2_data[no2_data<0] <- 0
no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
weather_data <- weather_cube %>%
slice(index = length(file_paths)-window_size+i, along = "time") %>%
pull()
weather_data <- weather_data[,40:1] / max_weather
cube_array[i,1,,] <- no2_data
cube_array[i,2,,] <- weather_data
cube_window[[i]] <- cube_array[i,,,]
cube_window[[i]][is.na(cube_window[[i]])] <- 0
cube_window[[i]][is.nan(cube_window[[i]])] <- 0
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
cube_window[[i]] <- cube_window[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
}
# Create an initial sequence the size of the window
cube_array <- array(0, dim = c(window_size, 2, ncol(raster::raster(file_paths[[1]])), nrow(raster::raster(file_paths[[1]]))))
cube_window <- vector("list", window_size)
for (i in 1:window_size){
no2_data <- read_stars(file_paths[[length(file_paths)-window_size+i]]) %>% pull()
no2_data <- no2_data[,40:1]
no2_data[no2_data<0] <- 0
no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
weather_data <- weather_cube %>%
slice(index = length(file_paths)-window_size+i, along = "time") %>%
pull()
weather_data <- weather_data[,40:1] / max_weather
cube_array[i,1,,] <- no2_data
cube_array[i,2,,] <- weather_data
cube_window[[i]] <- cube_array[i,,,]
cube_window[[i]][is.na(cube_window[[i]])] <- 0
cube_window[[i]][is.nan(cube_window[[i]])] <- 0
cube_window[[i]] <- cube_window[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
no2_data
no2_data %>% class()
no2_data <- no2_data %>% as.im() %>% blur(3) %>% as.matrix()
no2_data
no2_data %>% image()
# Create an initial sequence the size of the window
cube_array <- array(0, dim = c(window_size, 2, ncol(raster::raster(file_paths[[1]])), nrow(raster::raster(file_paths[[1]]))))
cube_window <- vector("list", window_size)
for (i in 1:window_size){
no2_data <- read_stars(file_paths[[length(file_paths)-window_size+i]]) %>% pull()
no2_data <- no2_data[,40:1]
no2_data[no2_data<0] <- 0
no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
no2_data <- no2_data %>% as.im() %>% blur(3) %>% as.matrix()
weather_data <- weather_cube %>%
slice(index = length(file_paths)-window_size+i, along = "time") %>%
pull()
weather_data <- weather_data[,40:1] / max_weather
cube_array[i,1,,] <- no2_data
cube_array[i,2,,] <- weather_data
cube_window[[i]] <- cube_array[i,,,]
cube_window[[i]][is.na(cube_window[[i]])] <- 0
cube_window[[i]][is.nan(cube_window[[i]])] <- 0
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
init_seq <- torch_stack(cube_window, dim = 1)
# a quick look into what we're validating with
cube_window[[30]][1,,] %>% as.matrix() %>% image()
cube_window[[30]][2,,] %>% as.matrix() %>% image()
# a quick look into what we're validating with
cube_window[[30]][1,,] %>% as.matrix() %>% image()
no2_data
no2_data %>% image()
# Read the files and combine into a single array
cube_array <- array(0, dim = c( length(file_paths)-1, 2, ncol(raster::raster(file_paths[[1]])), nrow(raster::raster(file_paths[[1]]))))
cube_rst <- vector("list", length(file_paths)-1)
for (i in seq_along(file_paths[1:length(file_paths)-1])){
no2_data <- read_stars(file_paths[[i]]) %>% pull()
no2_data <- no2_data[,40:1]
no2_data[no2_data<0] <- 0
no2_data[is.na(no2_data)] <- 0
no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
weather_data <- weather_cube %>% slice(index = i, along = "time") %>% pull()
weather_data <- weather_data[,40:1]
cube_array[i,1,,] <- no2_data
cube_array[i,2,,] <- weather_data
cube_rst[[i]] <- cube_array[i,,,]
cube_rst[[i]][is.na(cube_rst[[i]])] <- 0
cube_rst[[i]][is.nan(cube_rst[[i]])] <- 0
cube_rst[[i]] <- cube_rst[[i]] %>% torch_tensor()
}
# The data may not have a pair length, which complicates for the windows
# For this reason, we'll compute the remainder and not use the beginning of the
# time series, but only the "end"
beamed_size <-  as.integer(length(cube_rst) / window_size)
remainder <- length(cube_rst) %% beamed_size + 1
cube_rst <- cube_rst[1:(length(cube_rst)-remainder)]
cube_beams <- vector("list", beamed_size)
cube_beams[[1]] <- init_seq # beginning of our windows data will be forced
for (i in 2:beamed_size){
from = (i - 1) * window_size + 1
to = from + window_size - 1 # i am still ignoring 361
cube_beams[[i]] <- cube_rst[from:to] %>% torch_stack(., 1)
}
input <- torch_stack(cube_beams, dim = 2)
# input <- input$unsqueeze(3) # create artificial dimension to squeeze into input format
input %>% dim()
rm(cube_rst, cube_beams)
gc()
data_size = dim(input)[2]
trn_size = data_size - 1
# Create dataloaders from the datasets
train_ds <- create_train_ds(input)
rm(input)
gc()
train_dl <- dataloader(train_ds, batch_size = window_size)
preds_wet <- train_convlstm(train_dl, train_dl, num_epochs = 120,
.device = dev,
lr = 0.001, # 0.0001
input_dim = 2,
hidden_dims = c(64, 1),
kernel_sizes = c(3, 3),
n_layers = 2,
)
# Check Forecast
preds_wet[30,1,,] %>% as.matrix() %>% image()
val <- cube_window[[30]][1,,] %>% as.matrix()
# plot the real value
# Reshape the matrix for ggplot
prd_df <- melt(val)
ggplot(prd_df, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_viridis_c(limits = c(0,.4)) +
labs(x = "X", y = "Y", title = "NO2 0-1") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
guides(fill = guide_colorbar(title = "NO2 Real Value"))
val %>% image()
cube_window[[30]]
cube_window[[30]][1,,]
cube_window[[30]][1,,] %>% as.matrix() %>% image()
cube_window[[30]][2,,] %>% as.matrix() %>% image()
cube_window[[30]][1,,] %>% as.matrix() %>% image()
cube_window[[1]][1,,] %>% as.matrix() %>% image()
cube_window[[30]][1,,] %>% as.matrix() %>% image()
# Create an initial sequence the size of the window
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
cube_window[[i]] <- cube_window[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
cube_window[[i]][is.na(cube_window[[i]])] <- 0
cube_window[[i]][is.nan(cube_window[[i]])] <- 0
cube_window[[i]][cube_window[[i]]<0] <- 0
cube_window[[i]][cube_window[[i]]>100] <- 0 # rough outlier removal
cube_window[[i]] <- cube_window[[i]] / max_v # scaling from 0 to 1 (original_value - min_value) / (max_value - min_value)
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
cube_window[[30]] %>% as.matrix() %>% image()
idx = length(file_paths)-window_size+i
read_stars(file_paths[[idx]])
read_stars(file_paths[[idx]]) %>% pot()
read_stars(file_paths[[idx]]) %>% plot()
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
no2_data <- read_stars(file_paths[[idx]]) %>% pull()
no2_data <- no2_data[,40:1]
no2_data[no2_data<0] <- 0
no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
no2_data <- no2_data %>% as.im() %>% blur(3) %>% as.matrix()
weather_data <- weather_cube %>%
slice(index = length(file_paths)-window_size+i, along = "time") %>%
pull()
weather_data <- weather_data[,40:1] / max_weather
cube_array[i,1,,] <- no2_data
cube_array[i,2,,] <- weather_data
cube_window[[i]] <- cube_array[i,,,]
cube_window[[i]][is.na(cube_window[[i]])] <- 0
cube_window[[i]][is.nan(cube_window[[i]])] <- 0
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
cube_window[[30]]
cube_window[[30]][1,,]
cube_window[[30]][1,,] %>% as.matrix() %>% image()
no2_data
no2_data %>% image()
no2_data <- read_stars(file_paths[[idx]]) %>% pull()
no2_data %>% image()
no2_data <- no2_data[,40:1]
no2_data %>% image()
no2_data[no2_data<0] <- 0
no2_data %>% image()
no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data %>% image()
no2_data <- no2_data / max_v
no2_data %>% image()
no2_data <- no2_data %>% as.im() %>% blur(3) %>% as.matrix()
no2_data %>% image()
# Create an initial sequence the size of the window
cube_window <- vector("list", window_size)
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
cube_window[[30]] %>% image()
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
cube_window[[i]] %>% image()
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
idx = length(file_paths)-window_size+i
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
cube_window[[i]] %>% image()
## Wrangle Cube to tensor
# Get all the file paths in the directory
file_paths <- list.files(path = "vignettes/data/switzerland",
pattern = "*\\.tif$", full.names = TRUE)
# Create an initial sequence the size of the window
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
cube_window[[i]] <- cube_window[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
cube_window[[i]][is.na(cube_window[[i]])] <- 0
cube_window[[i]][is.nan(cube_window[[i]])] <- 0
cube_window[[i]][cube_window[[i]]<0] <- 0
cube_window[[i]][cube_window[[i]]>100] <- 0 # rough outlier removal
cube_window[[i]] <- cube_window[[i]] / max_v # scaling from 0 to 1 (original_value - min_value) / (max_value - min_value)
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
cube_window[[30]] %>% image()
cube_window[[30]] %>% as.matrix() %>% image()
# Create an initial sequence the size of the window
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
cube_window[[i]] <- raster(file_paths[[idx]]) %>% as.matrix()
cube_window[[i]] <- cube_window[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
# cube_window[[i]][is.na(cube_window[[i]])] <- 0
# cube_window[[i]][is.nan(cube_window[[i]])] <- 0
# cube_window[[i]][cube_window[[i]]<0] <- 0
# cube_window[[i]][cube_window[[i]]>100] <- 0 # rough outlier removal
cube_window[[i]] <- cube_window[[i]] / max_v # scaling from 0 to 1 (original_value - min_value) / (max_value - min_value)
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
cube_window[[30]] %>% as.matrix() %>% image()
raster(file_paths[[idx]]) %>% as.matrix()
raster(file_paths[[idx]]) %>% as.matrix() %>% image()
# Create an initial sequence the size of the window
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
no2_data <- read_stars(file_paths[[idx]]) %>% pull()
no2_data <- no2_data[,40:1]
# no2_data[no2_data<0] <- 0
# no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
no2_data <- no2_data %>% as.im() %>% blur(3) %>% as.matrix()
cube_window[[i]] <- no2_data / max_v # scaling from 0 to 1
# cube_window[[i]][is.na(cube_window[[i]])] <- 0
# cube_window[[i]][is.nan(cube_window[[i]])] <- 0
# cube_window[[i]][cube_window[[i]]<0] <- 0
# cube_window[[i]][cube_window[[i]]>100] <- 0 # rough outlier removal
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
cube_window[[30]] %>% as.matrix() %>% image()
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
no2_data <- read_stars(file_paths[[idx]]) %>% pull()
no2_data <- no2_data[,40:1]
# no2_data[no2_data<0] <- 0
# no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
no2_data <- no2_data %>% as.im() %>% blur(3) %>% as.matrix()
cube_window[[i]] <- no2_data / max_v # scaling from 0 to 1
# cube_window[[i]][is.na(cube_window[[i]])] <- 0
# cube_window[[i]][is.nan(cube_window[[i]])] <- 0
# cube_window[[i]][cube_window[[i]]<0] <- 0
# cube_window[[i]][cube_window[[i]]>100] <- 0 # rough outlier removal
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
init_seq <- torch_stack(cube_window, dim = 1)
# Read the files and combine into a single array
cube_rst <- vector("list", length(file_paths)-1)
for (i in seq_along(file_paths[1:length(file_paths)-1])) {
cube_rst[[i]] <- raster(file_paths[[i]]) %>% as.matrix()
cube_rst[[i]][is.na(cube_rst[[i]])] <- 0
cube_rst[[i]][is.nan(cube_rst[[i]])] <- 0
cube_rst[[i]][cube_rst[[i]]<0] <- 0
cube_rst[[i]][cube_rst[[i]]>100] <- 0 # rough outlier removal
cube_rst[[i]] <- cube_rst[[i]] / max_v # scaling from 0 to 1
cube_rst[[i]] <- cube_rst[[i]] %>% torch_tensor()
}
# The data may not have a pair length, which complicates for the windows
# For this reason, we'll compute the remainder and not use the beginning of the
# time series, but only the "end"
beamed_size <-  as.integer(length(cube_rst) / window_size)
remainder <- length(cube_rst) %% beamed_size + 1
cube_rst <- cube_rst[1:(length(cube_rst)-remainder)]
cube_beams <- vector("list", beamed_size)
cube_beams[[1]] <- init_seq # beginning of our windows data will be forced
for (i in 2:beamed_size){
from = (i - 1) * window_size + 1
to = from + window_size - 1 # i am still ignoring 361
cube_beams[[i]] <- cube_rst[from:to] %>% torch_stack(., 1)
}
input <- torch_stack(cube_beams, dim = 2)
cube_window[[30]] %>% as.matrix() %>% image()
init_seq
# Read the files and combine into a single array
cube_rst <- vector("list", length(file_paths)-1)
for (i in seq_along(file_paths[1:length(file_paths)-1])) {
cube_rst[[i]] <- raster(file_paths[[i]]) %>% as.matrix()
cube_rst[[i]][is.na(cube_rst[[i]])] <- 0
cube_rst[[i]][is.nan(cube_rst[[i]])] <- 0
cube_rst[[i]][cube_rst[[i]]<0] <- 0
cube_rst[[i]][cube_rst[[i]]>100] <- 0 # rough outlier removal
cube_rst[[i]] <- cube_rst[[i]] / max_v # scaling from 0 to 1
cube_rst[[i]] <- cube_rst[[i]] %>% torch_tensor()
}
cube_rst[[i]] <- raster(file_paths[[i]]) %>% as.matrix()
cube_rst[[i]] <- cube_rst[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
# cube_rst[[i]][is.na(cube_rst[[i]])] <- 0
# cube_rst[[i]][is.nan(cube_rst[[i]])] <- 0
# cube_rst[[i]][cube_rst[[i]]<0] <- 0
# cube_rst[[i]][cube_rst[[i]]>100] <- 0 # rough outlier removal
# cube_rst[[i]] <- cube_rst[[i]] / max_v # scaling from 0 to 1
cube_rst[[i]] <- cube_rst[[i]] %>% torch_tensor()
# The data may not have a pair length, which complicates for the windows
# For this reason, we'll compute the remainder and not use the beginning of the
# time series, but only the "end"
beamed_size <-  as.integer(length(cube_rst) / window_size)
remainder <- length(cube_rst) %% beamed_size + 1
cube_rst <- cube_rst[1:(length(cube_rst)-remainder)]
cube_beams <- vector("list", beamed_size)
cube_beams[[1]] <- init_seq # beginning of our windows data will be forced
for (i in 2:beamed_size){
from = (i - 1) * window_size + 1
to = from + window_size - 1 # i am still ignoring 361
cube_beams[[i]] <- cube_rst[from:to] %>% torch_stack(., 1)
}
input <- torch_stack(cube_beams, dim = 2)
cube_beams
cube_beams[[1]]
cube_beams
cube_beams %>% dim()
cube_rst
remainder
beamed_size
from
to
cube_beams[[i]]
cube_beams[[i]] %>% as.matrix %>% image()
cube_beams[[i]][1,,] %>% as.matrix %>% image()
# Create an initial sequence the size of the window
cube_window <- vector("list", window_size)
for (i in 1:window_size){
idx = length(file_paths)-window_size+i
no2_data <- read_stars(file_paths[[idx]]) %>% pull()
no2_data <- no2_data[,40:1]
# no2_data[no2_data<0] <- 0
# no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data %>% as.im() %>% blur(3) %>% as.matrix()
cube_window[[i]] <- no2_data / max_v # scaling from 0 to 1
# cube_window[[i]][is.na(cube_window[[i]])] <- 0
# cube_window[[i]][is.nan(cube_window[[i]])] <- 0
# cube_window[[i]][cube_window[[i]]<0] <- 0
# cube_window[[i]][cube_window[[i]]>100] <- 0 # rough outlier removal
cube_window[[i]] <- cube_window[[i]] %>% torch_tensor()
}
cube_window[[i]] %>% as.matrix %>% as.image
cube_window[[i]] %>% as.matrix %>% as.image()
cube_window[[i]] %>% as.matrix %>% image()
init_seq <- torch_stack(cube_window, dim = 1)
# Read the files and combine into a single array
cube_rst <- vector("list", length(file_paths)-1)
for (i in seq_along(file_paths[1:length(file_paths)-1])) {
cube_rst[[i]] <- raster(file_paths[[i]]) %>% as.matrix()
cube_rst[[i]] <- cube_rst[[i]] %>% as.im() %>% blur(3) %>% as.matrix()
# cube_rst[[i]][is.na(cube_rst[[i]])] <- 0
# cube_rst[[i]][is.nan(cube_rst[[i]])] <- 0
# cube_rst[[i]][cube_rst[[i]]<0] <- 0
# cube_rst[[i]][cube_rst[[i]]>100] <- 0 # rough outlier removal
cube_rst[[i]] <- cube_rst[[i]] / max_v # scaling from 0 to 1
cube_rst[[i]] <- cube_rst[[i]] %>% torch_tensor()
}
cube_rst[[i]]
cube_rst[[i]] %>% as.matrix() %>% image()
cube_rst[[30]] %>% as.matrix() %>% image()
# Read the files and combine into a single array
cube_rst <- vector("list", length(file_paths)-1)
for (i in seq_along(file_paths[1:length(file_paths)-1])) {
no2_data <- read_stars(file_paths[[i]]) %>% pull()
no2_data <- no2_data[,40:1] %>% as.im() %>% blur(3) %>% as.matrix()
# no2_data[no2_data<0] <- 0
# no2_data[is.na(no2_data)] <- 0
# no2_data[no2_data>100] <- 100 # rough outlier removal
no2_data <- no2_data / max_v
cube_rst[[i]] <- no2_data  %>% torch_tensor()
}
cube_rst[[i]] %>% as.matrix %>% image
# The data may not have a pair length, which complicates for the windows
# For this reason, we'll compute the remainder and not use the beginning of the
# time series, but only the "end"
beamed_size <-  as.integer(length(cube_rst) / window_size)
remainder <- length(cube_rst) %% beamed_size + 1
cube_rst <- cube_rst[1:(length(cube_rst)-remainder)]
cube_beams <- vector("list", beamed_size)
cube_beams[[1]] <- init_seq # beginning of our windows data will be forced
for (i in 2:beamed_size){
from = (i - 1) * window_size + 1
to = from + window_size - 1
cube_beams[[i]] <- cube_rst[from:to] %>% torch_stack(., 1)
}
input <- torch_stack(cube_beams, dim = 2)
input <- input$unsqueeze(3) # create artificial dimension to squeeze into input format
input %>% dim()
rm(cube_rst, cube_beams)
gc()
data_size = dim(input)[2]
trn_size = data_size - 1
create_train_ds <- dataset(
initialize = function(data) {
self$data <- data
},
.getitem = function(i) {
list(x = self$data[i, 1:trn_size, ..], y = self$data[i, data_size, ..])
},
.length = function() {
nrow(self$data)
}
)
# Create dataloaders from the datasets
train_ds <- create_train_ds(input)
rm(input)
gc()
train_dl <- dataloader(train_ds, batch_size = window_size)
## Find Optimal Learning Rate
train_dl %>% find_lr(.device = dev,
input_dim = 1,
hidden_dims = c(64, 1),
kernel_sizes = c(3, 3),
n_layers = 2,
min_lr = 0.0000001,
max_lr = 0.007,
steps = 60,
num_epochs = 2)
