kernel_sizes = c(3, 3),
n_layers = 8)
# Quick Training for sanity check
preds <- train_convlstm(cube[[1]], cube[[2]], num_epochs = 20, .device = dev, lr = 0.0003,
input_dim = 1,
hidden_dims = c(36, 1),
kernel_sizes = c(3, 3),
n_layers = 2)
## Wrangle Cube to tensor
cube <- create_dl_from_cube(seq_len = 6, batch_size = 117)
# Quick Training for sanity check
preds <- train_convlstm(cube[[1]], cube[[2]], num_epochs = 20, .device = dev, lr = 0.0003,
input_dim = 1,
hidden_dims = c(36, 1),
kernel_sizes = c(3, 3),
n_layers = 2)
cube[[1]]$x
cube[[1]]$b
library(eoforecast)
# Quick Training for sanity check
preds <- train_convlstm(cube[[1]], cube[[2]], num_epochs = 20, .device = dev, lr = 0.0003,
input_dim = 1,
hidden_dims = c(64, 1),
kernel_sizes = c(3, 3),
n_layers = 2)
dev <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
## Wrangle Cube to tensor
cube <- create_dl_from_cube(seq_len = 6, batch_size = 117)
# Quick Training for sanity check
preds <- train_convlstm(cube[[1]], cube[[2]], num_epochs = 20, .device = dev, lr = 0.0003,
input_dim = 1,
hidden_dims = c(64, 1),
kernel_sizes = c(3, 3),
n_layers = 2)
train_dl = cube[[|]]
train_dl = cube[[1]]
val_dl = cube[[2]]
num_epochs = 10
num_epochs = 4
input_dim = 1
hidden_dims = c(64, 1)
kernel_sizes = c(3, 3)
n_layers = 2
lr = 0.0003
.device = dev
model <- convlstm(input_dim = input_dim, hidden_dims = hidden_dims, kernel_sizes = kernel_sizes, n_layers = n_layers)
model <- model$to(device = .device)
### Adam optimizer
optimizer <- optim_adam(model$parameters, lr = lr)
trn_loss <- c()
val_loss <- c()
epc <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_bullets(paste("Epoch:", epoch))
cli::cli_rule()
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in train_dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
preds %>% as.matrix() %>% image()
b$y %>% as.matrix %>% image()
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Training:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
### -------- Backpropagation --------
loss$backward()
### -------- Update weights --------
optimizer$step()
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
trn_loss <- c(trn_loss, mean(batch_losses, na.rm = T))
epc <- c(epc, epoch)
}
# Early stopping
if (epoch > 4 && mean(batch_losses, na.rm = TRUE) >= trn_loss[epoch - 4]) {
cat("Validation loss did not improve. Early stopping...\n")
break  # Stop training
} else {
val_loss <- c(val_loss, mean(batch_losses, na.rm = TRUE))
}
model$eval()
batch_losses <- c()
# disable gradient tracking to reduce memory usage
with_no_grad({
coro::loop(for (b in val_dl) {
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Validation:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
})
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, validation loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
val_loss <- c(val_loss, mean(batch_losses, na.rm = T))
}
}
preds
preds %>% as.matrix()
b
b$x
b$x %>% dim()
model(b$x)
model(b$x)[[2]][[2]][[1]]
round(as.matrix(preds[1, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(preds[50, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(preds[1, 1, 1:40, 1:91]), 2) %>% image()
b$y
round(as.matrix(b$y[1, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(preds[1, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[1, 1, 1:40, 1:91]), 2) %>% image()
raster("vignettes/data/switzerland/openEO_2019-01-03Z.tif") %>% image()
round(as.matrix(b$y[1, 1, 1:40, 1:91]), 2) %>% image()
raster("vignettes/data/switzerland/openEO_2019-01-03Z.tif") %>% image()
round(as.matrix(b$y[2, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[3, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[4, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[66, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[350, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[349, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[348, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[340, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[330, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[320, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[310, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[280, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[250, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[210, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[150, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[800, 1, 1:40, 1:91]), 2) %>% image()
round(as.matrix(b$y[80, 1, 1:40, 1:91]), 2) %>% image()
raster("vignettes/data/switzerland/openEO_2019-01-03Z.tif") %>% image()
round(as.matrix(b$y[1, 1, 1:91, 1:40]), 2) %>% image()
libray(eoforecast)
library(eoforecast)
beams <- vector(mode = "list", length = 6)
beam <- torch_eye(6) %>% nnf_pad(c(6, 12, 12, 6)) # left, right, top, bottom
beams[[1]] <- beam
for (i in 2:6) {
beams[[i]] <- torch_roll(beam, c(-(i-1),i-1), c(1, 2))
}
init_sequence <- torch_stack(beams, dim = 1)
sequences <- vector(mode = "list", length = 100)
sequences[[1]] <- init_sequence
for (i in 2:100) {
sequences[[i]] <- torchvision::transform_random_affine(init_sequence, degrees = 0, translate = c(0.5, 0.5))
}
input <- torch_stack(sequences, dim = 1)
# add channels dimension
input <- input$unsqueeze(3)
dim(input)
input
input[1,1,1,1:24, 1:24] %>% image()
input[1,1,1,1:24, 1:24] %>% as.matrix() %>% image()
input[2,1,1,1:24, 1:24] %>% as.matrix() %>% image()
input[3,1,1,1:24, 1:24] %>% as.matrix() %>% image()
input[3,1,1,1:24, 1:24] %>% as.matrix() %>% image()
input[4,1,1,1:24, 1:24] %>% as.matrix() %>% image()
input[5,1,1,1:24, 1:24] %>% as.matrix() %>% image()
input[6,1,1,1:24, 1:24] %>% as.matrix() %>% image()
input[6,1,1,1:24, 1:24] %>% as.matrix() %>% image()
dev <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
preds <- train_convlstm(train_dl = dl, val_dl = dl, 100, plot_path = "vignettes/learning_curve-sanity.png", .device = dev)
dl <- create_dummy_data()
preds <- train_convlstm(train_dl = dl, val_dl = dl, 100, plot_path = "vignettes/learning_curve-sanity.png", .device = dev)
mean(batch_losses, na.rm = TRUE)
batch_losses
train_dl = dl
val_dl = dl
num_epochs = 10
input_dim = 1
hidden_dims = c(64, 1)
kernel_sizes = c(3, 3)
n_layers = 2
lr = 0.001
.device = dev
model <- convlstm(input_dim = input_dim, hidden_dims = hidden_dims, kernel_sizes = kernel_sizes, n_layers = n_layers)
model <- model$to(device = .device)
### Adam optimizer
optimizer <- optim_adam(model$parameters, lr = lr)
trn_loss <- c()
val_loss <- c()
epc <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_bullets(paste("Epoch:", epoch))
cli::cli_rule()
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in train_dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) %>% image(main = "Prediction")
Sys.sleep(2)
round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2) %>% image(main = "Validation")
Sys.sleep(2)
(round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) - round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2)) %>%
image(main = "Error")
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Training:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
### -------- Backpropagation --------
loss$backward()
### -------- Update weights --------
optimizer$step()
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
trn_loss <- c(trn_loss, mean(batch_losses, na.rm = T))
epc <- c(epc, epoch)
}
# Early stopping
print(epoch)
print(trn_loss[epoch - 3])
print(trn_loss[1])
if (epoch > 4 && mean(batch_losses, na.rm = TRUE) >= trn_loss[epoch - 3]) {
cat("Validation loss did not improve. Early stopping...\n")
break  # Stop training
} else {
val_loss <- c(val_loss, mean(batch_losses, na.rm = TRUE))
}
model$eval()
batch_losses <- c()
# disable gradient tracking to reduce memory usage
with_no_grad({
coro::loop(for (b in val_dl) {
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Validation:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
})
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, validation loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
val_loss <- c(val_loss, mean(batch_losses, na.rm = T))
}
}
plot(1)
for (epoch in 1:num_epochs) {
cli::cli_bullets(paste("Epoch:", epoch))
cli::cli_rule()
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in train_dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) %>% image(main = "Prediction")
Sys.sleep(2)
round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2) %>% image(main = "Validation")
Sys.sleep(2)
(round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) - round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2)) %>%
image(main = "Error")
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Training:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
### -------- Backpropagation --------
loss$backward()
### -------- Update weights --------
optimizer$step()
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
trn_loss <- c(trn_loss, mean(batch_losses, na.rm = T))
epc <- c(epc, epoch)
}
# Early stopping
print(epoch)
print(trn_loss[epoch - 3])
print(trn_loss[1])
if (epoch > 4 && mean(batch_losses, na.rm = TRUE) >= trn_loss[epoch - 3]) {
cat("Validation loss did not improve. Early stopping...\n")
break  # Stop training
} else {
val_loss <- c(val_loss, mean(batch_losses, na.rm = TRUE))
}
model$eval()
batch_losses <- c()
# disable gradient tracking to reduce memory usage
with_no_grad({
coro::loop(for (b in val_dl) {
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Validation:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
})
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, validation loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
val_loss <- c(val_loss, mean(batch_losses, na.rm = T))
}
}
for (epoch in 1:num_epochs) {
cli::cli_bullets(paste("Epoch:", epoch))
cli::cli_rule()
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in train_dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
# round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) %>% image(main = "Prediction")
# Sys.sleep(2)
# round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2) %>% image(main = "Validation")
# Sys.sleep(2)
# (round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) - round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2)) %>%
#   image(main = "Error")
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Training:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
### -------- Backpropagation --------
loss$backward()
### -------- Update weights --------
optimizer$step()
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
trn_loss <- c(trn_loss, mean(batch_losses, na.rm = T))
epc <- c(epc, epoch)
}
# Early stopping
print(epoch)
print(trn_loss[epoch - 3])
print(trn_loss[1])
if (epoch > 4 && mean(batch_losses, na.rm = TRUE) >= trn_loss[epoch - 3]) {
cat("Validation loss did not improve. Early stopping...\n")
break  # Stop training
} else {
val_loss <- c(val_loss, mean(batch_losses, na.rm = TRUE))
}
model$eval()
batch_losses <- c()
# disable gradient tracking to reduce memory usage
with_no_grad({
coro::loop(for (b in val_dl) {
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Validation:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
})
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, validation loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
val_loss <- c(val_loss, mean(batch_losses, na.rm = T))
}
}
trn_loss <- c()
val_loss <- c()
epc <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_bullets(paste("Epoch:", epoch))
cli::cli_rule()
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in train_dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
# round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) %>% image(main = "Prediction")
# Sys.sleep(2)
# round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2) %>% image(main = "Validation")
# Sys.sleep(2)
# (round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) - round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2)) %>%
#   image(main = "Error")
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Training:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
### -------- Backpropagation --------
loss$backward()
### -------- Update weights --------
optimizer$step()
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
trn_loss <- c(trn_loss, mean(batch_losses, na.rm = T))
epc <- c(epc, epoch)
# Early stopping
print(trn_loss)
print(trn_loss[epoch - 40])
if (epoch >= 50 && mean(batch_losses, na.rm = TRUE) >= trn_loss[epoch - 40]) {
cat("Validation loss did not improve. Early stopping...\n")
break  # Stop training
} else {
val_loss <- c(val_loss, mean(batch_losses, na.rm = TRUE))
}
}
model$eval()
batch_losses <- c()
# disable gradient tracking to reduce memory usage
with_no_grad({
coro::loop(for (b in val_dl) {
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Validation:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
})
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, validation loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
val_loss <- c(val_loss, mean(batch_losses, na.rm = T))
}
}
dl <- create_dummy_data()
train_dl = dl
val_dl = dl
model <- convlstm(input_dim = input_dim, hidden_dims = hidden_dims, kernel_sizes = kernel_sizes, n_layers = n_layers)
model <- model$to(device = .device)
### Adam optimizer
optimizer <- optim_adam(model$parameters, lr = lr)
trn_loss <- c()
val_loss <- c()
epc <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
cli::cli_bullets(paste("Epoch:", epoch))
cli::cli_rule()
cli::cli_progress_update()
model$train()
batch_losses <- c()
coro::loop(for (b in train_dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
# round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) %>% image(main = "Prediction")
# Sys.sleep(2)
# round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2) %>% image(main = "Validation")
# Sys.sleep(2)
# (round(as.matrix(preds[1, 1, 1:91, 40:1]), 2) - round(as.matrix(b$y[1, 1, 1:91, 40:1]), 2)) %>%
#   image(main = "Error")
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Training:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
### -------- Backpropagation --------
loss$backward()
### -------- Update weights --------
optimizer$step()
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
trn_loss <- c(trn_loss, mean(batch_losses, na.rm = T))
epc <- c(epc, epoch)
# Early stopping
print(trn_loss)
print(trn_loss[epoch - 40])
if (epoch >= 50 && mean(batch_losses, na.rm = TRUE) >= trn_loss[epoch - 40]) {
cat("Validation loss did not improve. Early stopping...\n")
break  # Stop training
} else {
val_loss <- c(val_loss, mean(batch_losses, na.rm = TRUE))
}
}
model$eval()
batch_losses <- c()
# disable gradient tracking to reduce memory usage
with_no_grad({
coro::loop(for (b in val_dl) {
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
cli::cli_alert_info(paste("Validation:", loss$item(), "\n"))
batch_losses <- c(batch_losses, ifelse(is.infinite(loss$item()), NA, loss$item()))
})
})
# Print Loss
if (epoch %% 10 == 0){
cat(sprintf("\nEpoch %d, validation loss:%3f\n", epoch, mean(batch_losses, na.rm = T)))
val_loss <- c(val_loss, mean(batch_losses, na.rm = T))
}
}
library(eoforecast)
dev <- torch_device(if(cuda_is_available()) {"cuda"}else{"cpu"})
## Wrangle Cube to tensor
cube <- create_dl_from_cube(seq_len = 6, batch_size = 350)
## Wrangle Cube to tensor
cube <- create_dl_from_cube(seq_len = 6, batch_size = 100)
# Quick Training for sanity check
preds <- train_convlstm(cube[[1]], cube[[2]], num_epochs = 4, .device = dev,
lr = 0.001,
input_dim = 1,
hidden_dims = c(64, 1),
kernel_sizes = c(5, 5),
n_layers = 2)
