}
cat("downloading results")
try(dir.create(output_path))
download_results(job = job$id, folder = output_path)
cli::cli_progress_done()
cli::cli_alert_success('{.strong {func_name}}Download and processing {cli::col_green("successfull")}!')
cli::cli_rule();cli::cli_end()
}
download_s5p_from_openeo(country = "switzerland", date_ext = c("2019-01-01", "2019-03-01"), output_path = "~/msc-thesis/data/")
download_s5p_from_openeo <- function( country = NULL,
date_ext = list(),
n = NULL, e = NULL, s = NULL, w = NULL,
output_path=".", con = connect(host = "https://openeo.cloud"), p = processes()){
# mask for cloud cover
threshold_ = function(data, context) {
threshold = p$gte(data[1], 0.5)
return(threshold)
}
# interpolate where no data
interpolate = function(data,context) {
return(p$array_interpolate_linear(data = data))
}
# Moving average
ma = function(x){
filter(x,rep(1/31, 31))
}
# use bounding box
bbox = c(n, e, s, w)
if (is.null(country) & !is.null(bbox)){
n = bbox$n
e = bbox$e
s = bbox$s
w = bbox$w
# use country name
}else if(!is.null(country) & is.character(country) & is.null(bbox)){
country_sf = rnaturalearth::ne_countries(country = country, returnclass = "sf", scale = 'large')
n = sf::st_bbox(country_sf)[4]
e = sf::st_bbox(country_sf)[3]
s = sf::st_bbox(country_sf)[2]
w = sf::st_bbox(country_sf)[1]
# use country sf
}else if((!is.null(country) & inherits(country, "sf") & is.null(bbox))){
n = sf::st_bbox(country)[4]
e = sf::st_bbox(country)[3]
s = sf::st_bbox(country)[2]
w = sf::st_bbox(country)[1]
}else{
stop(cli::format_error("Either pass a country in sf of character, or the coordinates. Never both!"))
}
# acquire data for the extent
datacube_no2 = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=date_ext,
bands=c("NO2")
)
datacube = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=date_ext,
bands=c("CLOUD_FRACTION")
)
# apply the threshold to the cube
cloud_threshold <- p$apply(data = datacube, process = threshold_)
# mask the cloud cover with the calculated mask
datacube <- p$mask(datacube_no2, cloud_threshold)
datacube = p$apply_dimension(process = interpolate,
data = datacube, dimension = "t"
)
cli::cli_rule(left = "Download S5P data", right = "{lubridate::now()}");cli::cli_end()
formats = list_file_formats()
result = p$save_result(data = datacube, format = formats$output$GeoTiff)
job = create_job(graph=result, title = "s5p")
start_job(job = job)
jobs = list_jobs()
cli::cli_progress_bar("Downloading data", type = "download")
while (jobs[[job$id]]$status == 'running' | jobs[[job$id]]$status == 'queued' | jobs[[job$id]]$status == 'created' ){
print(paste0('this may take a moment, your process is ', jobs[[job$id]]$status))
cli::cli_progress_update()
Sys.sleep(60)
jobs = list_jobs()
if (jobs[[job$id]]$status == 'finished' | jobs[[job$id]]$status == 'error'){
break
}
}
cat("downloading results")
try(dir.create(output_path))
download_results(job = job$id, folder = output_path)
cli::cli_progress_done()
cli::cli_alert_success('{.strong {func_name}}Download and processing {cli::col_green("successfull")}!')
cli::cli_rule();cli::cli_end()
}
download_s5p_from_openeo(country = "switzerland", date_ext = c("2019-01-01", "2019-02-01"), output_path = "~/msc-thesis/data/")
download_s5p_from_openeo <- function( country = NULL,
date_ext = list(),
n = NULL, e = NULL, s = NULL, w = NULL,
output_path=".", con = connect(host = "https://openeo.cloud"), p = processes()){
# mask for cloud cover
threshold_ = function(data, context) {
threshold = p$gte(data[1], 0.5)
return(threshold)
}
# interpolate where no data
interpolate = function(data,context) {
return(p$array_interpolate_linear(data = data))
}
# Moving average
ma = function(x){
filter(x,rep(1/31, 31))
}
# use bounding box
bbox = c(n, e, s, w)
if (is.null(country) & !is.null(bbox)){
n = bbox$n
e = bbox$e
s = bbox$s
w = bbox$w
# use country name
}else if(!is.null(country) & is.character(country) & is.null(bbox)){
country_sf = rnaturalearth::ne_countries(country = country, returnclass = "sf", scale = 'large')
n = sf::st_bbox(country_sf)[4]
e = sf::st_bbox(country_sf)[3]
s = sf::st_bbox(country_sf)[2]
w = sf::st_bbox(country_sf)[1]
# use country sf
}else if((!is.null(country) & inherits(country, "sf") & is.null(bbox))){
n = sf::st_bbox(country)[4]
e = sf::st_bbox(country)[3]
s = sf::st_bbox(country)[2]
w = sf::st_bbox(country)[1]
}else{
stop(cli::format_error("Either pass a country in sf of character, or the coordinates. Never both!"))
}
# acquire data for the extent
datacube_no2 = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=date_ext,
bands=c("NO2")
)
datacube = p$load_collection(
id = "SENTINEL_5P_L2",
spatial_extent = list(west = w, south = s, east = e, north = n),
temporal_extent=date_ext,
bands=c("CLOUD_FRACTION")
)
# apply the threshold to the cube
cloud_threshold <- p$apply(data = datacube, process = threshold_)
# mask the cloud cover with the calculated mask
datacube <- p$mask(datacube_no2, cloud_threshold)
datacube = p$apply_dimension(process = interpolate,
data = datacube, dimension = "t"
)
cli::cli_rule(left = "Download S5P data", right = "{lubridate::now()}");cli::cli_end()
formats = list_file_formats()
result = p$save_result(data = datacube, format = formats$output$GeoTiff)
job = create_job(graph=result, title = "s5p")
start_job(job = job)
jobs = list_jobs()
while (jobs[[job$id]]$status == 'running' | jobs[[job$id]]$status == 'queued' | jobs[[job$id]]$status == 'created' ){
print(paste0('this may take a moment, your process is ', jobs[[job$id]]$status))
Sys.sleep(60)
jobs = list_jobs()
if (jobs[[job$id]]$status == 'finished' | jobs[[job$id]]$status == 'error'){
break
}
}
cli::cli_progress_bar("Downloading data", type = "download")
try(dir.create(output_path))
download_results(job = job$id, folder = output_path)
cli::cli_progress_done()
cli::cli_alert_success('{.strong {func_name}}Download and processing {cli::col_green("successfull")}!')
cli::cli_rule();cli::cli_end()
}
download_s5p_from_openeo(country = "switzerland", date_ext = c("2019-01-01", "2019-02-01"), output_path = "~/msc-thesis/data/")
file.path()
file.path("~/msc-thesis/eoforecast/vignettes/data/switzerland/")
path_to_tiffs = file.path("~/msc-thesis/eoforecast/vignettes/data/switzerland/")
# Moving average
ma = function(x){
filter(x,rep(1/31, 31))
}
# list files and build cube
ls = list.files(path_to_tiffs, pattern = ".tif", all.files = T, full.names = T)
ls
# list files and build cube
ls = list.files(path_to_tiffs, pattern = ".tif", all.files = F, full.names = T)
ls
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
library(dplyr)
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
sf::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
?st_apply
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
?list.files
# list files and build cube
ls = list.files(path_to_tiffs, pattern = ".tif", all.files = F, full.names = T) %>%
filter(ends_with('.tif'))
# list files and build cube
ls = list.files(path_to_tiffs, pattern = "\.tif$", all.files = F, full.names = T)
# list files and build cube
ls = list.files(path_to_tiffs, pattern = "\\.tif$", all.files = F, full.names = T)
ls
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
# list files and build cube
ls = list.files(path_to_tiffs, pattern = "\\.tif$", all.files = F, full.names = T) %>%
file.path()
ls
path_to_tiffs
path_to_tiffs = "~/msc-thesis/eoforecast/vignettes/data/switzerland"
# list files and build cube
ls = list.files(path_to_tiffs, pattern = "\\.tif$", all.files = F, full.names = T) %>%
file.path()
ls
# list files and build cube
ls = list.files(path_to_tiffs, pattern = "\\.tif$", all.files = F, full.names = T)
ls
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
cube = stars::read_stars(ls)
ls[1]
# local moving average
cube = stars::read_stars(ls[1]) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
raw_cube = stars::read_stars(ls) %>%
merge()
cube
stars::read_stars(ls)
# local moving average
cube = stars::read_stars(ls, dimensions = c("x", "y", "band")) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
raw_cube = stars::read_stars(ls, dimensions = c("x", "y", "band")) %>%
merge()
raster::raster(ls[1])
stars::read_stars(ls[1])
?read_stars()
opts = list(dimensions = c("x", "y", "band"), overwrite = TRUE)
# local moving average
cube = stars::read_stars(ls, options = opts) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
install.packages("stars")
install.packages("stars")
library(stars)
stars::read_stars(ls[1])
stars::read_stars(ls[1])
ls[1]
read_stars("./vignettes/data/switzerland/openEO_2019-01-01Z.tif")
devtools::install_github("https://r-spatial.github.io/stars/")
devtools::install_github("https://r-spatial.github.io/stars")
remotes::install_github("r-spatial/stars")
library(stars)
ls
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
stars::read_stars(ls[1])
ls[1]
# local moving average
tif = system.file(ls[1], package = "stars")
tif
cube = stars::read_stars(tif) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
ls[1]
# local moving average
tif = system.file("/home/hr/msc-thesis/eoforecast/vignettes/data/switzerland/openEO_2019-01-01Z.tif", package = "stars")
tif
# local moving average
tif = system.file("/home/hr/msc-thesis/data/openEO_2019-01-01Z.tif", package = "stars")
tif
cube = stars::read_stars(tif) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
path_to_tiffs
path_to_tiffs = "~/msc-thesis/data/openEO_2019-01-01Z.tif"
path_to_tiffs = "~/msc-thesis/data"
# list files and build cube
ls = list.files(path_to_tiffs, pattern = "\\.tif$", all.files = F, full.names = T)
ls
# local moving average
cube = stars::read_stars(ls) %>%
merge() %>%
stars::st_apply(c("x", "y"), ma) %>%
aperm(c(2,3,1))
(stars::read_stars(ls))
read_stars("/home/hr/msc-thesis/eoforecast/vignettes/data/switzerland/openEO_2019-01-01Z.tif")
(x1 = read_stars("/home/hr/msc-thesis/eoforecast/vignettes/data/switzerland/openEO_2019-01-01Z.tif"))
tif = system.file("/home/hr/msc-thesis/eoforecast/vignettes/data/switzerland/openEO_2019-01-01Z.tif", package = "stars")
tif
tif = system.file("tif/L7_ETMs.tif", package = "stars")
tif
(x1 = read_stars(tif))
train_dl
# NO2 Forecasting using Sentinel 5P and Weather Forecast data: a Deep Learning Approach
library(eoforecast)
# Read as Array
cube = create_dl_from_cube()
cube %>% length()
train_dl <- cube[1]
test_dl <- cube[2]
preds <- train_convlstm(train_dl, 3)
train_dl
path_to_tiffs = "vignettes/data/switzerland"
batch_size = 100
train_pct = 0.8
seq_len = 12
# Get all the file paths in the directory
file_paths <- list.files(path = path_to_tiffs, pattern = "*\\.tif$", full.names = TRUE)
# Read the files and combine into a single array
cube_array <- array(0, dim = c(length(file_paths), 1, nrow(raster::raster(file_paths[[1]])), ncol(raster::raster(file_paths[[1]]))))
for (i in seq_along(file_paths)) {
cube_array[i,,,] <- raster(file_paths[[i]])[]
}
# Create sequence tensor
cube_array_seq <- array(0, dim = c(dim(cube_array)[1] - seq_len + 1, seq_len, dim(cube_array)[2], dim(cube_array)[3], dim(cube_array)[4]))
for (i in 1:(dim(cube_array)[1] - seq_len + 1)) {
cube_array_seq[i,, , ,] <- cube_array[i:(i + seq_len - 1), , , ]
}
# Replace NAs and NaNs with 0
cube_array_seq[is.na(cube_array_seq)] <- 0
cube_array_seq[is.nan(cube_array_seq)] <- 0
# convert to tensor
cube_tensor <- torch_tensor(cube_array_seq, dtype = torch_float())
# Split into training and validation sets
train_size <- floor(dim(cube_tensor)[1] * train_pct)
train_data <- cube_tensor[1:train_size, , , , ]
val_data <- cube_tensor[(train_size+1):dim(cube_tensor)[1], , , , ]
# Create dummy dataset object from the tensor
dummy_ds <- dataset(
initialize = function(data) {
self$data <- data
},
.getitem = function(i) {
list(x = self$data[i, , , , ], y = self$data[i, seq_len, , , ])
},
.length = function() {
dim(self$data)[1]
}
)
# Create dataloaders from the datasets
train_ds <- dummy_ds(train_data)
train_dl <- dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)
val_ds <- dummy_ds(val_data)
val_dl <- dataloader(val_ds, batch_size = batch_size, shuffle = FALSE)
preds <- train_convlstm(train_dl, 3)
library(eoforecast)
as.Date("2023-10-09") - as.Date("2023-05-21")
140 /7
20 * 6
## convLSTM
library(eoforecast)
# dummy input tensor
x <- torch_rand(c(2, 4, 3, 16, 16)) # batch_size, seq_len, channels, height, width
## Sanity Check
dl <- create_dummy_data()
preds <- train_convlstm(dl = dl, 10, plot_path = "vignettes/learning_curve-sanity.png")
dl
dl %>% size()
dl %>% dim()
preds
preds %>% dim()
dl
dl$sampler
dl %>% length()
cube <- readRDS("~/msc-thesis/cube_ch.rds")
train_dl <- cube[1]
test_dl <- cube[2]
preds <- train_convlstm(train_dl, 3)
train_dl
cube
## Wrangle Cube to tensor
cube = create_dl_from_cube()
cube
train_dl <- cube[1]
test_dl <- cube[2]
preds <- train_convlstm(train_dl, 3)
train_dl
test_dl
cube
train_dl <- cube[[1]]
test_dl <- cube[[2]]
train_dl
preds <- train_convlstm(train_dl, 3)
library(eoforecast)
## convLSTM
library(eoforecast)
# Quick Training for sanity check
preds <- train_convlstm(train_dl, 3)
# dummy input tensor
x <- torch_rand(c(2, 4, 3, 16, 16)) # batch_size, seq_len, channels, height, width
## Sanity Check
dl <- create_dummy_data()
preds <- train_convlstm(dl = dl, 10, plot_path = "vignettes/learning_curve-sanity.png")
## Tiny convlstm
train_convlstm <- function(dl,
num_epochs = 100,
plot_path = "vignettes/learning_curve.png",
input_dim = 1,
hidden_dims = c(64, 1),
kernel_sizes = c(3, 3),
n_layers = 2){
model <- convlstm(input_dim = input_dim, hidden_dims = hidden_dims, kernel_sizes = kernel_sizes, n_layers = n_layers)
### Adam optimizer
optimizer <- optim_adam(model$parameters)
losses <- c()
## Loop through Epochs
cli::cli_progress_bar("Training convlstm", total = num_epochs)
for (epoch in 1:num_epochs) {
model$train()
batch_losses <- c()
coro::loop(for (b in dl) {
optimizer$zero_grad()
# last-time-step output from last layer
preds <- model(b$x)[[2]][[2]][[1]]
loss <- nnf_mse_loss(preds, b$y)
losses <- c(losses, as.numeric(loss))
cli::cli_alert(paste("Loss:", (loss %>% as.numeric() %>% round(3))))
batch_losses <- c(batch_losses, loss$item())
loss$backward()
optimizer$step()
})
if (epoch %% 10 == 0)
cat(sprintf("\nEpoch %d, training loss:%3f\n", epoch, mean(batch_losses)))
cli::cli_progress_update()
}
cli::cli_progress_done()
# create learning rate plot
learning_curve <- tibble(epoch = 1:num_epochs, loss = losses) %>%
ggplot(aes(x = epoch, y = loss)) +
geom_line() +
xlab("Epochs") +
ylab("Loss") +
ggtitle("Learning Rate Curve")
ggsave(plot_path, learning_curve)
return(preds)
}
train_dl
train_dl <- cube[[1]]
train_dl
cube
## Wrangle Cube to tensor
cube = create_dl_from_cube()
train_dl <- cube[[1]]
test_dl <- cube[[2]]
train_dl
# Quick Training for sanity check
preds <- train_convlstm(train_dl, 3)
rlang::last_trace()
library(eoforecast)
## convLSTM
library(eoforecast)
# dummy input tensor
x <- torch_rand(c(2, 4, 3, 16, 16)) # batch_size, seq_len, channels, height, width
## Sanity Check
dl <- create_dummy_data()
## Wrangle Cube to tensor
cube = create_dl_from_cube()
library(eoforecast)
install.packages("openeo")
library(eoforecast)
library(openeo)
install.packages("sf")
library(eoforecast)
install.packages("openeo")
library(eoforecast)
library(openeo)
remove.packages("openeo")
remove.packages("sf")
.libPaths()
install.packages("sf")
install.packages("openeo")
library(sfheaders)
library(sf)
library(sf)
install.packages("sf", dep = T)
library(sf)
install.packages("sf", dep = T)
install.packages("satellite", dep = T)
library(eoforecast)
